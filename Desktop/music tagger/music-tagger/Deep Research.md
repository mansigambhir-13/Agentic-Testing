# Comprehensive Notes on Audio/Music Tagging Models (2016–2025)

## CNN-Based Music Tagging Models (2016–2019)

- **CRNN (Convolutional Recurrent Neural Network, 2016):** An early deep learning approach for music tagging that combines convolutional layers for spectral feature extraction with recurrent layers (e.g. GRUs) for temporal context[arxiv.org](https://arxiv.org/abs/1609.04243#:~:text=,feature%20extraction%20and%20feature%20summarisation). Keunwoo Choi _et al._ demonstrated that a CRNN model can achieve strong tag prediction performance (ROC-AUC ≈0.866) on music tagging benchmarks while using relatively few parameters[github.com](https://github.com/keunwoochoi/music-auto_tagging-keras#:~:text=). Trained on datasets like MagnaTagATune and Million Song Dataset tags, CRNNs became a **baseline architecture** for music auto-tagging tasks, capturing both local timbral cues and song-level temporal structure[arxiv.org](https://arxiv.org/abs/1609.04243#:~:text=,feature%20extraction%20and%20feature%20summarisation)[github.com](https://github.com/keunwoochoi/music-auto_tagging-keras#:~:text=).

- **PANNs (Pretrained Audio Neural Networks, 2019):** A suite of CNN models pretrained on the large-scale AudioSet corpus (5000+ hours, 527 classes) for general audio tagging[github.com](https://github.com/qiuqiangkong/audioset_tagging_cnn#:~:text=This%20repo%20contains%20code%20for,art%20systems). PANNs introduced a novel _Wavegram-Logmel CNN_ architecture that inputs both raw waveform and mel-spectrogram, achieving **state-of-the-art** tagging results on AudioSet (mAP 0.439) and outperforming prior systems (previous best mAP 0.392)[signalprocessingsociety.org](https://signalprocessingsociety.org/events/sps-webinar-panns-large-scale-pretrained-audio-neural-networks-audio-pattern-recognition#:~:text=convolutional%20neural%20networks%20and%20propose,in%20several%20of%20those%20tasks). These models transfer effectively to various audio tasks (sound events, music, speech) and are available as open-source pretrained models[github.com](https://github.com/qiuqiangkong/audioset_tagging_cnn#:~:text=This%20repo%20contains%20code%20for,art%20systems). _Significance:_ PANNs established a new high-water mark on AudioSet in 2019 and provided generalizable audio feature extractors for downstream tasks[signalprocessingsociety.org](https://signalprocessingsociety.org/events/sps-webinar-panns-large-scale-pretrained-audio-neural-networks-audio-pattern-recognition#:~:text=convolutional%20neural%20networks%20and%20propose,in%20several%20of%20those%20tasks).


## Transformer-Based Advances in Audio Tagging (2021–2022)

- **AST (Audio Spectrogram Transformer, 2021):** The first convolution-free, purely attention-based model for audio classification[huggingface.co](https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer#:~:text=model,accuracy%20on%20Speech%20Commands%20V2). AST applies a Vision Transformer (ViT) architecture to 2D audio spectrogram patches, capturing long-range context with self-attention. Trained on AudioSet, AST achieved **new SOTA performance**: e.g. **mAP 0.485 on AudioSet** (an improvement over prior CNN models), 95.6% accuracy on ESC-50, and 98.1% on Speech Commands V2[huggingface.co](https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer#:~:text=model,accuracy%20on%20Speech%20Commands%20V2). Its success showed that transformers (with sufficient data) can outperform CNNs on audio tasks by modeling global relationships in the sound.

- **HTS-AT (Hierarchical Token-Semantic Audio Transformer, 2022):** An improved audio transformer that uses a _hierarchical structure_ (reducing token length at stages) plus a token-semantic mapping module[arxiv.org](https://arxiv.org/abs/2202.00874#:~:text=the%20model%27s%20scalability%20in%20audio,model%20parameters%20and%2015). HTS-AT is much more efficient than vanilla transformers, using only ~35% of the parameters and 15% of the training time of prior audio transformers[arxiv.org](https://arxiv.org/abs/2202.00874#:~:text=event%20detection%20%28i,AT). It achieved **state-of-the-art results on AudioSet and ESC-50** in 2022, while matching the SOTA on Speech Commands V2[arxiv.org](https://arxiv.org/abs/2202.00874#:~:text=event%20detection%20%28i,AT). Additionally, HTS-AT’s token-semantic module produces class-wise activation maps, enabling sound _event localization_ (temporal tagging) better than previous CNNs[arxiv.org](https://arxiv.org/abs/2202.00874#:~:text=event%20detection%20%28i,AT). _Bottom line:_ HTS-AT offered SOTA accuracy with a fraction of the compute, thanks to hierarchical attention pooling.

- **MAE-AST (Masked Autoencoding Audio Spectrogram Transformer, 2023):** A self-supervised extension of AST inspired by Masked Autoencoders. MAE-AST integrates an encoder–decoder architecture where the **encoder processes only unmasked spectrogram patches** and a lightweight decoder reconstructs the masked patches[arxiv.org](https://arxiv.org/pdf/2203.16691#:~:text=tokens,SSAST%20on%20a%20variety%20of)[arxiv.org](https://arxiv.org/pdf/2203.16691#:~:text=in%20significant%20increases%20in%20computational,audio%20spectrogram%20domain%20intro%02duces%20generally). This design yields a 3× speedup and 2× memory reduction in pre-training compared to prior Self-Supervised AST (SSAST) models[arxiv.org](https://arxiv.org/pdf/2203.16691#:~:text=Learners%20,conduct%20comprehensive%20evalua%02tions%20into%20different). When fine-tuned, MAE-AST **outperforms the original SSAST** on multiple audio classification tasks[arxiv.org](https://arxiv.org/pdf/2203.16691#:~:text=reduction%20over%20the%20vanilla%20SSAST,the%20visual%20and%20audio%20domains), while being more computationally efficient. _In essence_, MAE-AST demonstrates the benefit of masked spectrogram prediction for learning robust audio representations.

- **Other Transformer Variants:** _Swin-Transformer models_ have also been applied. For example, **S3T (Self-Supervised Swin Transformer, 2022)** uses a MoCo-based contrastive training with a Swin Transformer backbone to learn music representations[arxiv.org](https://arxiv.org/abs/2202.10139#:~:text=,Experimental)[arxiv.org](https://arxiv.org/abs/2202.10139#:~:text=classification,labeled%20data). S3T achieved **12.5% higher genre classification accuracy and +4.8% PR-AUC** in tagging compared to previous self-supervised models, even surpassing some task-specific supervised baselines[arxiv.org](https://arxiv.org/abs/2202.10139#:~:text=classification,labeled%20data). This underscores that transformer backbones (ViT, Conformer, Swin, etc.) paired with large-scale training have become dominant in recent audio tagging research.


## Self-Supervised Audio Representation Models (2022–2025)

- **M2D (Masked Modeling Duo, 2024):** A self-supervised framework that improves on masked reconstruction methods for audio[arxiv.org](https://arxiv.org/abs/2404.06095#:~:text=%3E%20Abstract%3ASelf,distribution%20from%20that%20in%20pre). M2D uses a _dual-network approach_: one network encodes only the masked segments of audio while another encodes the unmasked parts, and the model learns by predicting the latent representation of the masked portion[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2404.06095v1#:~:text=Self,distribution%20from%20that%20in%20pre)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2404.06095v1#:~:text=representation%20robust,code%20is%20available%20online%20for). By **encoding only masked regions**, M2D forces the representation to truly model the audio content (avoiding trivial copying from visible parts)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2404.06095v1#:~:text=representations%20of%20masked%20input%20signals,representations%20for%20an%20application%20X)[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2404.06095v1#:~:text=representation%20robust,code%20is%20available%20online%20for). This approach produced highly general audio embeddings – experiments showed **top-level performance on AudioSet** tagging as well as speech and even a small medical dataset, demonstrating M2D’s universal applicability[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2404.06095v1#:~:text=representation%20specialized%20to%20serve%20various,code%20is%20available%20online%20for). _Key point:_ M2D’s representations, learned from ~2M AudioSet clips (and FSD50k), set a new standard for general-purpose audio pre-training.

- **M2D-CLAP (Masked Modeling Duo meets CLAP, 2024):** A multimodal extension combining M2D’s audio encoder with **Contrastive Language–Audio Pre-training (CLAP)**[isca-archive.org](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.pdf#:~:text=Masked%20Modeling%20Duo%20,validate%20our%20proposal%20with%20many). The goal is a **general-purpose audio–language representation** that is useful both for transfer learning on audio tasks and for zero-shot text-to-audio inference[isca-archive.org](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.pdf#:~:text=AM%20that%20uses%20masked%20prediction,unified%20test%20environment%20and%20compare). M2D-CLAP trains an audio encoder (initialized by M2D) alongside a text encoder so that their embeddings align, akin to CLIP[isca-archive.org](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.pdf#:~:text=Masked%20Modeling%20Duo%20,purpose%20audio%02language%20representation.%20We%20validate). The result is a model that achieved **high supervised tagging performance** (matching state-of-the-art audio models) _and_ strong zero-shot classification ability comparable to the best audio-language models[isca-archive.org](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.pdf#:~:text=learning,language%20representation%2C%20ii%29%20proposal%20of). In other words, M2D-CLAP can act as a regular audio tagger or a zero-shot tagger. This was validated on diverse benchmarks, showing that integrating self-supervised audio learning with audio-text alignment yields a truly versatile model[isca-archive.org](https://www.isca-archive.org/interspeech_2024/niizumi24_interspeech.pdf#:~:text=Experiments%20show%20high%20transfer%20learning,also%20release%20our%20code%20and).

- **M2D2 (Second-Generation M2D, 2025):** An advanced model building on M2D-CLAP. **M2D2 uses a two-stage training**: first learn general audio features (via M2D) and align them with text using a CLAP objective, then refine using a powerful text encoder (incorporating an LLM-based sentence embedding for rich semantics)[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=audio%20features%20and%20CLAP%20features,training%20stages%2C%20M2D2%20should%20enhance)[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=text%29%20in%20a%20two,0%20for%20AudioSet%2C%20SOTA). The use of a Large Language Model text encoder in training provided stronger semantic supervision than earlier CLAP models[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=propose%20M2D2%2C%20a%20second,Experiments). M2D2 set a new milestone, achieving **SOTA fine-tuned performance on AudioSet (mAP 49.0)**[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=generalizability%20and%20performance%20in%20its,language%20tasks) – the highest reported to date – as well as state-of-the-art results on music tagging tasks (e.g. MagnaTagATune) and robust performance on audio-text retrieval[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=generalizability%20and%20performance%20in%20its,language%20tasks). _Impact:_ M2D2 is currently one of the best general audio tagging models, and its audio-text embeddings further enable rich cross-modal applications.

- **MuQ (Mel Residual Vector Quantization, 2025):** A self-supervised music representation model from Tencent AI Lab focused on **compact discrete representation** of music audio[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=representation%20learning%20model%20for%20music,160K%20hours%20and%20adopting%20iterative). MuQ is trained to predict tokens obtained by a _Mel-residual vector quantizer (Mel-RVQ)_, which directly quantizes mel-spectrogram frames using a residual codebook approach[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=proposed%20model%2C%20named%20MuQ%2C%20is,text%20embedding%20model). This yields stable targets for self-supervised learning, as opposed to using random projections or off-the-shelf audio codecs. Despite using only ~0.9K hours of open data for pre-training, MuQ **outperformed prior music SSL models** on a wide range of tasks (tagging, instrument ID, key detection, etc.)[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=projection%20structure%20for%20Mel%20spectrum,Code%20and). Scaling pre-training to 160k hours further boosted performance[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=large%20variety%20of%20downstream%20tasks,source%20in%20this%20https%20URL). Moreover, the authors combined MuQ with a text model (via MuLan’s contrastive scheme) to create **MuQ-MuLan**, a joint audio–text model. MuQ-MuLan achieved **state-of-the-art zero-shot music tagging** on MagnaTagATune[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=pre,source%20in%20this%20https%20URL), highlighting the effectiveness of MuQ’s audio tokens for cross-modal learning.

- **Audio–Text Embedding Models (MuLan, 2022):** In 2022, Google introduced **MuLan**, a two-tower model jointly embedding music audio and natural-language descriptions[research.google](https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/#:~:text=generation%20of%20acoustic%20models%20that,music%20tagging%2C%20language%20understanding%20in). MuLan was trained on an unprecedented scale: 44 million music tracks (≈370k hours) with _weak_ text annotations (e.g. metadata, comments)[research.google](https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/#:~:text=generation%20of%20acoustic%20models%20that,music%20tagging%2C%20language%20understanding%20in). The model learns a shared embedding space such that audio clips and their corresponding text are nearby. This enables **zero-shot music tagging and retrieval** – essentially “query by text” for audio. MuLan’s audio encoder (a modified CNN) and text encoder (Transformer) produce embeddings that subsume traditional tag ontologies, allowing flexible descriptions beyond fixed tag sets[research.google](https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/#:~:text=generation%20of%20acoustic%20models%20that,music%20tagging%2C%20language%20understanding%20in). _Outcome:_ MuLan demonstrated that large-scale audio–language training can yield very general representations; e.g. it powers MusicLM’s text-conditioned music generation by providing the audio-text similarity metric[aibusiness.com](https://aibusiness.com/nlp/google-unveils-musiclm-an-ai-text-to-music-model#:~:text=Google%20Unveils%20AI%20Text,fidelity%20music%20from%20text%20inputs). Subsequent works like MuZinger and CLAP built on this idea, but MuLan remains a seminal effort in aligning music audio with the richness of natural language.


## Graph-Based and Few-Shot Tagging Techniques

- **MT-GCN (Multi-Task Graph Convolutional Network, 2020):** An early approach incorporating label graphs into audio tagging. MT-GCN was designed to leverage the **ontology of sound classes** (like the AudioSet hierarchy) to learn label relationships and combat noisy labels[yifangyin.github.io](https://yifangyin.github.io/publications/ICASSP-20.Harsh.pdf#:~:text=such%20as%20the%20AudioSet%20are,relationships%20between%20sound%20events%20in)[yifangyin.github.io](https://yifangyin.github.io/publications/ICASSP-20.Harsh.pdf#:~:text=We%20therefore%20present%20MT,methods%20by%20a%20significant%20margin). It uses a GCN that learns an embedding for each tag based on the ontology graph, and integrates this with a CNN audio encoder in a multi-task setup (one task on a small clean set, one on a large noisy set)[yifangyin.github.io](https://yifangyin.github.io/publications/ICASSP-20.Harsh.pdf#:~:text=We%20therefore%20present%20MT,Terms%E2%80%94%20Audio%20Tagging%2C%20Graph%20Convolutional)[yifangyin.github.io](https://yifangyin.github.io/publications/ICASSP-20.Harsh.pdf#:~:text=Multi,The%20key). In experiments on the FSDKaggle2019 challenge (80-class noisy tagging), MT-GCN significantly outperformed baseline CNNs, showing the value of modeling inter-tag correlations[yifangyin.github.io](https://yifangyin.github.io/publications/ICASSP-20.Harsh.pdf#:~:text=two%20ontology,methods%20by%20a%20significant%20margin). _Key insight:_ by sharing information among related labels (e.g. “Flute” and “Woodwind”), the model became more robust to label noise and data sparsity[yifangyin.github.io](https://yifangyin.github.io/publications/ICASSP-20.Harsh.pdf#:~:text=We%20therefore%20present%20MT,methods%20by%20a%20significant%20margin).

- **LC-Prototypical Networks (Few-Shot Tagging, 2024):** A method to handle scenarios with _new tags and few examples_. LC-Protonets introduce the idea of generating one prototype **per combination of labels** rather than per label[arxiv.org](https://arxiv.org/abs/2409.11264#:~:text=%3E%20Abstract%3AWe%20introduce%20Label,The%20results%20demonstrate%20a%20significant). In a few-shot episode, all unique label-sets among the support examples are treated as distinct classes in a prototypical network[arxiv.org](https://arxiv.org/abs/2409.11264#:~:text=must%20generalize%20to%20new%20classes,shot). This approach was evaluated on _world music_ audio tagging across diverse cultural genres/instruments (a domain with many infrequent label combinations). **Results:** LC-Protonets delivered significantly higher accuracy than standard few-shot classifiers in nearly all test domains[arxiv.org](https://arxiv.org/abs/2409.11264#:~:text=automatic%20audio%20tagging%20across%20diverse,in%20contrast%20to%20the%20comparative). Even without any fine-tuning on a new domain, it achieved strong performance, whereas other methods required fine-tuning to catch up[arxiv.org](https://arxiv.org/abs/2409.11264#:~:text=existing%20approaches%20in%20the%20literature,The%20implementation%20and). Fine-tuning further improved all models, but LC-Protonets remained best-in-class[arxiv.org](https://arxiv.org/abs/2409.11264#:~:text=Protonets%20for%20multi,offering%20a%20benchmark%20for%20future). _Conclusion:_ modeling label combinations explicitly is highly effective for few-shot multi-label problems (where co-occurrence patterns matter).

- **Classifier Chains for Tag Dependencies (2025):** Hasumi _et al._ proposed modeling tag dependence via **Classifier Group Chains** for music tagging[arxiv.org](https://arxiv.org/abs/2501.05050#:~:text=interplay%20of%20music%20tags,We%20evaluate%20the%20effectiveness). Instead of predicting all tags independently, their method groups tags by semantic category (e.g. genre, mood, instrument) and then _chains_ the prediction of these groups in sequence[arxiv.org](https://arxiv.org/abs/2501.05050#:~:text=problems,tagging%20performance%20through%20music%20tagging). Within each group, tags are predicted jointly, and later groups’ models can condition on earlier groups’ outputs. This captures conditional dependencies (e.g. knowing the genre can inform instrument likelihoods) that flat models ignore[arxiv.org](https://arxiv.org/abs/2501.05050#:~:text=interplay%20of%20music%20tags,Our%20method%20allows). Experiments on the MTG-Jamendo tagging dataset showed improved tag prediction F-scores using classifier chains versus independent models (exact performance numbers are in the paper). They also analyzed different chain orderings to find the optimal sequence (genre→instrument→mood yielded good results) – highlighting that tag dependency modeling can boost accuracy in multi-label music tagging.

- **LHGNN (Local-Higher-Order GNN, 2025):** A new graph neural network model for general audio classification and tagging that aims to capture **higher-order relations** in audio data[arxiv.org](https://arxiv.org/abs/2501.03464#:~:text=leveraging%20self,shows%20that%20it%20outperforms%20Transformer). LHGNN addresses a limitation of Transformers: self-attention captures pairwise relations well but may miss higher-order structure (like complex interactions among three or more sound components)[arxiv.org](https://arxiv.org/abs/2501.03464#:~:text=leveraging%20self,shows%20that%20it%20outperforms%20Transformer). The model works by first clustering audio features (using fuzzy c-means) to identify local groupings, and then building a graph where nodes represent both individual time frames and these cluster centroids[arxiv.org](https://arxiv.org/abs/2501.03464#:~:text=introduces%20the%20Local,extensive%20pretraining%20data%20is%20unavailable). A GNN is used to propagate information on this graph, blending local neighborhood info with global cluster-level context[arxiv.org](https://arxiv.org/abs/2501.03464#:~:text=introduces%20the%20Local,extensive%20pretraining%20data%20is%20unavailable). The result is a richer feature representation of the audio. LHGNN outperformed Transformer models on three public audio datasets **with substantially fewer parameters**[arxiv.org](https://arxiv.org/abs/2501.03464#:~:text=information%20with%20higher,extensive%20pretraining%20data%20is%20unavailable). Notably, it excelled when no large-scale pretraining was available, suggesting superior efficiency in data-sparse settings[arxiv.org](https://arxiv.org/abs/2501.03464#:~:text=three%20publicly%20available%20audio%20datasets,extensive%20pretraining%20data%20is%20unavailable). _Big picture:_ LHGNN indicates that graph-based architectures can complement or even beat Transformers by explicitly modeling higher-order structure in audio signals.

- **Graph-Based Sample Identification (2025):** Bhattacharjee _et al._ developed a system for **music sample identification** – detecting reused audio snippets in songs – using a self-supervised GNN[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=,essential%20capability%20absent%20in%20prior). Their model encodes audio segments into embeddings via a lightweight GNN and contrastive learning (learning to group augmentations of the same sample)[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=,candidate%20selection%2C%20followed%20by%20a). Despite using only ~9% of the parameters of the previous SOTA, the GNN encoder reached comparable retrieval performance (mAP ≈44.2%) in identifying sampled audio clips[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=employing%20a%20Graph%20Neural%20Network,essential%20capability%20absent%20in%20prior). They further introduced a novel two-stage pipeline: (1) a coarse search to find candidate matches, and (2) a refined re-ranking using a _cross-attention classifier_ to filter false positives and reorder results[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=the,as%20part%20of%20this%20work). This second stage, absent in prior work, greatly improved precision in retrieval[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=,as%20part%20of%20this%20work). The system is robust to common audio transformations (time-stretch, pitch-shift, added effects), addressing a key challenge in sample identification[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=identification%20of%20portions%20of%20audio,is%20an%20important%20open%20challenge). _Outcome:_ This GNN-based approach (to appear at ISMIR 2025) pushes the state-of-the-art in automatic sample identification, while being more efficient than earlier CNN-based methods.


## Multimodal & Specialized Music Understanding Models (2025)

- **MuFun (Foundation Model for Music, 2025):** A large-scale _unified_ model aiming for holistic music understanding[arxiv.org](https://arxiv.org/abs/2508.01178#:~:text=,Experiments%20show%20our%20model). MuFun features a hybrid architecture that processes **audio (instrumentals) and lyrics together**, bridging the gap between traditionally separate MIR tasks[arxiv.org](https://arxiv.org/abs/2508.01178#:~:text=specialized%20models%20excelling%20at%20isolated,genre%20classification%2C%20music%20tagging%2C%20and). It was pre-trained on a massive proprietary dataset (reported as 13+ million hours of audio, plus lyrics) spanning tasks from genre and tagging to music _question answering_[arxiv.org](https://arxiv.org/abs/2508.01178#:~:text=music%20understanding,art%20effectiveness%20and%20generalization%20ability). To evaluate such broad capabilities, the authors introduced **MuCUE**, a benchmark covering multiple facets of music understanding[arxiv.org](https://arxiv.org/abs/2508.01178#:~:text=dataset%20covering%20diverse%20tasks%20such,art%20effectiveness%20and%20generalization%20ability). MuFun achieved **state-of-the-art results across MuCUE**, dramatically outperforming prior audio-language models on classification, tagging, and QA tasks[arxiv.org](https://arxiv.org/abs/2508.01178#:~:text=benchmark%20for%20multi,art%20effectiveness%20and%20generalization%20ability). This suggests MuFun learned a general internal representation of music that can be specialized to many applications. In essence, MuFun is proposed as a _foundation model_ for music, analogous to foundation models in vision or NLP, capable of being fine-tuned or prompted for virtually any music intelligence task.

- **Semantic-Aware Interpretable Tagger (2025):** Patakis _et al._ developed an **interpretable multimodal music auto-tagger** that combines audio features, lyrics, and knowledge from music ontology[arxiv.org](https://arxiv.org/abs/2505.17233#:~:text=performance%20in%20this%20domain%2C%20their,making). The system extracts groups of features that are musically meaningful – e.g. timbral features from signal processing, deep learned features, linguistic features from lyrics, and even high-level descriptors from an ontology (like genres or eras)[arxiv.org](https://arxiv.org/abs/2505.17233#:~:text=present%20an%20interpretable%20framework%20for,centric%20music%20tagging%20systems). These feature groups are clustered by semantic relatedness, and an expectation-maximization weighting assigns each group a contribution weight for a given tag decision[arxiv.org](https://arxiv.org/abs/2505.17233#:~:text=present%20an%20interpretable%20framework%20for,centric%20music%20tagging%20systems). The result is a tag prediction accompanied by an explanation of which feature groups (harmony, rhythm, lyrical content, etc.) contributed most. This model achieved **competitive tagging accuracy** (near state-of-the-art) while providing transparency into the decision process[arxiv.org](https://arxiv.org/abs/2505.17233#:~:text=deep%20learning%2C%20ontology%20engineering%2C%20and,centric%20music%20tagging%20systems). _Significance:_ It paves the way for more _explainable_ music recommender and tagging systems where a user could see _why_ a song was tagged as, say, “energetic” (e.g. “fast tempo and upbeat lyrics” features had high influence).

- **ChordFormer (Conformer for Chord Recognition, 2025):** A model tackling **automatic chord transcription** for large chord vocabularies[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=and%20exhibit%20suboptimal%20performance%20on,increase%20in). ChordFormer uses a _Conformer_ architecture (CNN + Transformer layers) to capture both local musical context and long-range harmonic dependencies[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=to%20tackle%20structural%20chord%20recognition,providing%20robust%20and%20balanced%20recognition). It also incorporates music-theory knowledge through structured chord representations (predicting chord root, quality, bass note, etc.) and addresses class imbalance via re-weighted loss (since complex chords are rare)[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=convolutional%20neural%20networks%20with%20transformers%2C,vocabulary). Evaluated on expanded chord datasets (beyond basic major/minor), ChordFormer **outperformed previous SOTA by ~2% (absolute) frame-wise accuracy and ~6% class-wise accuracy** for multi-class chord transcription[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=challenges%20such%20as%20class%20imbalance,vocabulary%20chord%20recognition). It proved especially robust on rare chord types, thanks to the architecture and loss adjustments[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=convolutional%20neural%20networks%20with%20transformers%2C,vocabulary). This model advances chord recognition toward more “musically complete” vocabularies (including 7ths, jazz chords, etc.), bridging a gap between academic MIR research and real-world music analysis needs[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=challenges%20such%20as%20class%20imbalance,vocabulary%20chord%20recognition).

- **OmniVec2 (Multimodal Multitask Model, 2025):** A state-of-the-art **transformer-based network for large-scale multimodal learning**, introduced as _“OmniVec2.”_ According to reports, OmniVec2 is trained across dozens of datasets and 12 modalities (text, image, audio, video, etc.) simultaneously[huggingface.co](https://huggingface.co/papers/trending#:~:text=site%20%28https%3A%2F%2Fomniflatten,Oct%2023%2C%202024)[main--dasarpai.netlify.app](https://main--dasarpai.netlify.app/dsblog/paperwithcode-resources/#:~:text=Explanation%3B%20Canonical%20Voting%3A%20Towards%20Robust,Oriented%20Bounding). For audio, it has been benchmarked on AudioSet and achieved **top-ranking performance** on the PapersWithCode Audio Classification leaderboard (AudioSet mAP ~49–50%). Essentially, OmniVec2 represents the trend of massive multimodal models that learn audio representations in conjunction with other modalities. While details are in an upcoming publication, the key point is that _as of 2025, the best AudioSet tagging results are coming from extremely large-scale models like OmniVec2 (multimodal) and specialist models like M2D2 (audio-language)_.


## **Current SOTA Highlights (2025):** Several of the above models define the state-of-the-art on major benchmarks:

- **AudioSet Tagging:** _M2D2_ and _OmniVec2_ are the current top performers on AudioSet, each with mean average precision around 0.49 (significantly above prior ~0.439 from 2019)[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=generalizability%20and%20performance%20in%20its,language%20tasks). These models benefit from massive data and advanced architectures (audio-language training for M2D2[arxiv.org](https://arxiv.org/html/2503.22104v1#:~:text=generalizability%20and%20performance%20in%20its,language%20tasks), and expansive multitask learning for OmniVec2).

- **Music Tagging (Music Genre/Mood):** _M2D2-AS+ (2025 variant)_ is reported as the best model on the MagnaTagATune dataset (a popular music tagging benchmark), reflecting the gains from audio-language pretraining even on pure audio tag tasks. Similarly, _HTS-AT (2022)_ and _MuQ-MuLan (2025)_ hold state-of-the-art results on datasets like MTG-Jamendo and MagnaTagATune for traditional (supervised) and zero-shot tagging respectively[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=pre,source%20in%20this%20https%20URL).

- **Zero-Shot Tagging/Retrieval:** _MuLan (2022)_ pioneered this area, and _MuQ-MuLan (2025)_ now achieves SOTA zero-shot tagging on MagnaTagATune[arxiv.org](https://arxiv.org/abs/2501.01108#:~:text=pre,source%20in%20this%20https%20URL) by combining a strong audio encoder with language embeddings. These enable tagging music with arbitrary text descriptors. Foundation models like _MuFun (2025)_ are also pushing the envelope by handling free-form music Q&A and description tasks, far beyond fixed tag prediction[arxiv.org](https://arxiv.org/abs/2508.01178#:~:text=dataset%20covering%20diverse%20tasks%20such,art%20effectiveness%20and%20generalization%20ability).

- **Specialized Tasks:** In music structure analysis, _ChordFormer (2025)_ is a new SOTA for chord transcription (especially on extended chord vocabularies)[arxiv.org](https://arxiv.org/abs/2502.11840#:~:text=challenges%20such%20as%20class%20imbalance,vocabulary%20chord%20recognition). For audio copy-detection (sample ID), the 2025 GNN approach matched prior SOTA with 1/10th the model size[arxiv.org](https://arxiv.org/abs/2506.14684#:~:text=employing%20a%20Graph%20Neural%20Network,essential%20capability%20absent%20in%20prior), indicating efficiency gains. And in the realm of interpretability, the Semantic-Aware tagger sets a precedent by delivering high accuracy with human-interpretable reasoning[arxiv.org](https://arxiv.org/abs/2505.17233#:~:text=present%20an%20interpretable%20framework%20for,centric%20music%20tagging%20systems).

